image:
  name: 644454719059.dkr.ecr.us-east-1.amazonaws.com/datalabs-bitbucket-pipelines:1.1.0
  aws:
    access-key: $AWS_ACCESS_KEY_ID
    secret-key: $AWS_SECRET_ACCESS_KEY

pipelines:
    branches:
        master:
        - step:
            name: Build and Auto-Merge to dev
            image: python:3.7
            caches:
                - build-dependencies
            script:
            - apt update
            - apt install -y jq
            - bash Script/bitbucket-pipelines-build.sh
            - git config remote.origin.fetch "+refs/heads/*:refs/remotes/origin/*"
            - git fetch
            - git checkout dev
            - git merge -m 'Automatic merge from master to dev.' $BITBUCKET_COMMIT
            - git push origin dev
        dev:
        - step:
            name: Build dev
            image: python:3.7
            caches:
                - build-dependencies
            script:
                - apt update
                - apt install -y jq
                - bash Script/bitbucket-pipelines-build.sh
        test:
        - step:
            name: Build test
            image: python:3.7
            caches:
                - build-dependencies
            script:
                - apt update
                - apt install -y jq
                - bash Script/bitbucket-pipelines-build.sh
        stage:
        - step:
            name: Build stage
            image: python:3.7
            caches:
                - build-dependencies
            script:
                - apt update
                - apt install -y jq
                - bash Script/bitbucket-pipelines-build.sh
        prod:
        - step:
            name: Build prod
            script:
                - apt update
                - apt install -y jq
                - bash Script/bitbucket-pipelines-build.sh
    custom:
        ##############################################################
        # global targets
        ##############################################################

        Test:
        - step:
            name: Unit and Lint Tests
            image: python:3.7
            caches:
                - test-dependencies
            script:
                - apt update
                - apt install -y jq
                - bash Script/bitbucket-pipelines-test.sh


        ##############################################################
        # master branch targets
        ##############################################################

        CPT-master:
        - parallel:
            # - step:
            #     name: Create Database
            #     caches:
            #       - cpt-master-create-database-dependencies
            #     script:
            #     - bash Script/setup-virtual-environment CPT
            #     - export ENABLE_FEATURE_DEV=True
            #     - export DATABASE_NAME=cpt
            #     - export DATABASE_NAME_ADMIN=postgres
            #     - export DATABASE_BACKEND=postgresql+psycopg2
            #     - export DATABASE_HOST=$CPT_DATABASE_HOST_SANDBOX
            #     - export DATABASE_PORT=5432
            #     - export DATABASE_USERNAME=$CPT_DATABASE_USERNAME_SANDBOX
            #     - export DATABASE_PASSWORD=$CPT_DATABASE_PASSWORD_SANDBOX
            #     - bash Deploy/CPT/create-database
            - step:
                name: Update Bundle
                caches:
                    - cpt-master-update-bundle-dependencies
                script:
                - bash Script/setup-virtual-environment Master/BitBucketPipelines
                - export ENABLE_FEATURE_DEV=True
                - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
                - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
                - export AWS_DEFAULT_REGION='us-east-1'
                - bash Deploy/CPT/create-bundle CPT
                - export AWS_S3_BUCKET='ama-sbx-datalake-lambda-us-east-1'
                - bash Deploy/CPT/upload-bundle
        - parallel:
            # - step:
            #     name: Migrate Database
            #     caches:
            #       - cpt-master-create-database-dependencies
            #     script:
            #     - bash Script/setup-virtual-environment Master/BitBucketPipelines
            #     - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
            #     - export PATH="$VIRTUAL_ENV/bin:$PATH"
            #     - bash Script/migrate-database --host $CPT_DATABASE_HOST_SANDBOX --name cpt --username $CPT_DATABASE_USERNAME_SANDBOX --password $CPT_DATABASE_PASSWORD_SANDBOX upgrade CPT
            - step:
                name: Update Lambda Functions
                script:
                - export ENABLE_FEATURE_DEV=True
                - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
                - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
                - export AWS_DEFAULT_REGION='us-east-1'
                - export ENVIRONMENT=sbx
                - bash Deploy/CPT/sbx/update-functions

        Scheduler-master:
        - step:
            name: Update Bundle
            caches:
            - scheduler-master-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Create the code bundle
            - export ENABLE_FEATURE_DEV=True
            - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
            - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
            - export AWS_DEFAULT_REGION='us-east-1'
            - bash Deploy/Master/create-bundle Scheduler

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-sbx-datalake-lambda-us-east-1'
            - bash Deploy/Master/upload-bundle Scheduler
        - step:
            name: Update Lambda Functions
            script:
            - export ENABLE_FEATURE_DEV=True
            - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
            - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
            - export AWS_DEFAULT_REGION='us-east-1'
            - export ENVIRONMENT=sbx
            - bash Deploy/Scheduler/update-functions

        OneView-master:
        - parallel:
            - step:
                name: Create Database
                caches:
                  - oneview-master-create-database-dependencies
                script:
                - bash Script/setup-virtual-environment OneView
                - export ENABLE_FEATURE_DEV=True
                - export DATABASE_NAME=oneview_content
                - export DATABASE_NAME_ADMIN=postgres
                - export DATABASE_BACKEND=postgresql+psycopg2
                - export DATABASE_HOST=$ONEVIEW_DATABASE_HOST_SANDBOX
                - export DATABASE_PORT=5432
                - export DATABASE_USERNAME=$ONEVIEW_DATABASE_USERNAME_SANDBOX
                - export DATABASE_PASSWORD=$ONEVIEW_DATABASE_PASSWORD_SANDBOX
                - bash Deploy/Master/create-database OneView
            - step:
                name: Update ETL Bundle
                caches:
                    - oneview-master-update-bundle-dependencies
                script:
                # Setup virtual environment
                - bash Script/setup-virtual-environment Master/BitBucketPipelines
                - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
                - export PATH="$VIRTUAL_ENV/bin:$PATH"

                # Install the DB2 JDBC driver
                - wget --no-check-certificate https://dbschema.com/jdbc-drivers/Db2JdbcDriver.zip
                - unzip ./Db2JdbcDriver.zip

                # Install the Informix JDBC driver
                - wget --no-check-certificate https://dbschema.com/jdbc-drivers/InformixJdbcDriver.zip
                - unzip ./InformixJdbcDriver.zip
                - echo "db2.jcc.charsetDecoderEncoder=3" >> ./DB2JccConfiguration.properties

                # Create the code bundle
                - export ENABLE_FEATURE_DEV=True
                - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
                - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
                - export AWS_DEFAULT_REGION='us-east-1'
                - bash Deploy/Master/create-bundle OneView/ETL db2jcc4.jar DB2JccConfiguration.properties jdbc-4.50.4.1.jar bson-4.2.0.jar

                # Upload the code bundle
                - export AWS_S3_BUCKET='ama-sbx-datalake-lambda-us-east-1'
                - bash Deploy/Master/upload-bundle OneView/ETL
            - step:
                name: Update API Bundle
                caches:
                    - oneview-master-update-bundle-dependencies
                script:
                # Setup virtual environment
                - bash Script/setup-virtual-environment Master/BitBucketPipelines
                - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
                - export PATH="$VIRTUAL_ENV/bin:$PATH"

                # Create the code bundle
                - export ENABLE_FEATURE_DEV=True
                - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
                - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
                - export AWS_DEFAULT_REGION='us-east-1'
                - bash Deploy/Master/create-bundle OneView/API

                # Upload the code bundle
                - export AWS_S3_BUCKET='ama-sbx-datalake-lambda-us-east-1'
                - bash Deploy/Master/upload-bundle OneView/API
        - parallel:
            # - step:
            #     name: Migrate Database
            #     caches:
            #       - oneview-master-create-database-dependencies
            #     script:
            #     - bash Script/setup-virtual-environment Master/BitBucketPipelines
            #     - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
            #     - export PATH="$VIRTUAL_ENV/bin:$PATH"
            #     - Script/run.py Script/migrate-database --host $ONEVIEW_DATABASE_HOST_SANDBOX --name oneview --username $ONEVIEW_DATABASE_USERNAME_SANDBOX --password $ONEVIEW_DATABASE_PASSWORD_SANDBOX upgrade OneView
            - step:
                name: Update Lambda Functions
                script:
                - export ENABLE_FEATURE_DEV=True
                - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
                - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
                - export AWS_DEFAULT_REGION='us-east-1'
                - export ENVIRONMENT=sbx
                - bash Deploy/OneView/update-functions


        ##############################################################
        # dev branch targets
        ##############################################################

        CPT-dev:
        - step:
            name: Update Bundle
            caches:
                - cpt-dev-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh dev | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_DEV=True
            - bash Deploy/Master/create-bundle CPT

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-dev-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/CPT/upload-bundle
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh dev | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_DEV=True
            - export ENVIRONMENT=dev
            - bash --rcfile assume_role.rc -i Deploy/CPT/dev/update-functions

        Scheduler-dev:
        - step:
            name: Update Bundle
            caches:
                - scheduler-dev-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh dev | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_DEV=True
            - bash Deploy/Master/create-bundle Scheduler

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-dev-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/Master/upload-bundle Scheduler
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh dev | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_DEV=True
            - export ENVIRONMENT=dev
            - bash --rcfile assume_role.rc -i Deploy/Scheduler/update-functions

        OneView-dev:
        - parallel:
            # - step:
            #     name: Create Database
            #     caches:
            #       - oneview-dev-create-database-dependencies
            #     script:
            #     - bash Script/setup-virtual-environment OneView
            #     - export ENABLE_FEATURE_DEV=True
            #     - export DATABASE_NAME=oneview_content
            #     - export DATABASE_NAME_ADMIN=postgres
            #     - export DATABASE_BACKEND=postgresql+psycopg2
            #     - export DATABASE_HOST=$ONEVIEW_DATABASE_HOST_DEV
            #     - export DATABASE_PORT=5432
            #     - export DATABASE_USERNAME=$ONEVIEW_DATABASE_USERNAME_DEV
            #     - export DATABASE_PASSWORD=$ONEVIEW_DATABASE_PASSWORD_DEV
            #     - bash Deploy/Master/create-database OneView
            - step:
                name: Update ETL Bundle
                caches:
                    - oneview-dev-update-bundle-dependencies
                script:
                # Setup virtual environment
                - bash Script/setup-virtual-environment Master/BitBucketPipelines
                - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
                - export PATH="$VIRTUAL_ENV/bin:$PATH"

                # Install the DB2 JDBC driver
                - wget --no-check-certificate https://dbschema.com/jdbc-drivers/Db2JdbcDriver.zip
                - unzip ./Db2JdbcDriver.zip
                - echo "db2.jcc.charsetDecoderEncoder=3" >> ./DB2JccConfiguration.properties

                # Install the Informix JDBC driver
                - wget --no-check-certificate https://dbschema.com/jdbc-drivers/InformixJdbcDriver.zip
                - unzip ./InformixJdbcDriver.zip

                # Assume maintenance role
                - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
                - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
                - aws configure set profile.apigw.region us-east-1
                - aws configure set profile.apigw.output json
                - bash Script/apigw_assume_role.sh dev | grep source > assume_role.rc

                # Create the code bundle
                - export ENABLE_FEATURE_DEV=True
                - bash Deploy/Master/create-bundle OneView/ETL db2jcc4.jar DB2JccConfiguration.properties jdbc-4.50.4.1.jar bson-4.2.0.jar

                # Upload the code bundle
                - export AWS_S3_BUCKET='ama-dev-datalake-lambda-us-east-1'
                - bash --rcfile assume_role.rc -i Deploy/Master/upload-bundle OneView/ETL
            # - step:
            #     name: Update API Bundle
            #     caches:
            #         - oneview-dev-update-bundle-dependencies
            #     script:
            #     # Setup virtual environment
            #     - bash Script/setup-virtual-environment Master/BitBucketPipelines
            #     - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
            #     - export PATH="$VIRTUAL_ENV/bin:$PATH"
            #
            #     # Assume maintenance role
            #     - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            #     - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            #     - aws configure set profile.apigw.region us-east-1
            #     - aws configure set profile.apigw.output json
            #     - bash Script/apigw_assume_role.sh dev | grep source > assume_role.rc
            #
            #     # Create the code bundle
            #     - export ENABLE_FEATURE_DEV=True
            #     - bash Deploy/Master/create-bundle OneView/API
            #
            #     # Upload the code bundle
            #     - export AWS_S3_BUCKET='ama-dev-datalake-lambda-us-east-1'
            #     - bash --rcfile assume_role.rc -i Deploy/Master/upload-bundle OneView/API
        - parallel:
            # - step:
            #     name: Migrate Database
            #     caches:
            #       - oneview-dev-create-database-dependencies
            #     script:
            #     - bash Script/setup-virtual-environment Master/BitBucketPipelines
            #     - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
            #     - export PATH="$VIRTUAL_ENV/bin:$PATH"
            #     - bash Script/migrate-database --host $ONEVIEW_DATABASE_HOST_DEV --name oneview --username $ONEVIEW_DATABASE_USERNAME_DEV --password $ONEVIEW_DATABASE_PASSWORD_DEV upgrade OneView
            - step:
                name: Update Lambda Functions
                script:
                # Assume maintenance role
                - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
                - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
                - aws configure set profile.apigw.region us-east-1
                - aws configure set profile.apigw.output json
                - bash Script/apigw_assume_role.sh dev | grep source > assume_role.rc

                # Update Lambda functions
                - export ENABLE_FEATURE_DEV=True
                - export ENVIRONMENT=dev
                - bash --rcfile assume_role.rc -i Deploy/OneView/update-functions


        ##############################################################
        # test branch targets
        ##############################################################

        CPT-test:
        - step:
            name: Update Bundle
            caches:
                - cpt-test-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_TEST=True
            - bash Deploy/Master/create-bundle CPT

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-tst-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/CPT/upload-bundle
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_TEST=True
            - export ENVIRONMENT=tst
            - bash --rcfile assume_role.rc -i Deploy/CPT/tst/update-functions

        Scheduler-test:
        - step:
            name: Update Bundle
            caches:
                - scheduler-test-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_TEST=True
            - bash Deploy/Master/create-bundle Scheduler

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-tst-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/Master/upload-bundle Scheduler
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_TEST=True
            - export ENVIRONMENT=tst
            - bash --rcfile assume_role.rc -i Deploy/Scheduler/update-functions

        OneView-test:
        - step:
            name: Update ETL Bundle
            caches:
                - oneview-test-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines
            - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
            - export PATH="$VIRTUAL_ENV/bin:$PATH"

            # Install the DB2 JDBC driver
            - wget --no-check-certificate https://dbschema.com/jdbc-drivers/Db2JdbcDriver.zip
            - unzip ./Db2JdbcDriver.zip
            - echo "db2.jcc.charsetDecoderEncoder=3" >> ./DB2JccConfiguration.properties

            # Install the Informix JDBC driver
            - wget --no-check-certificate https://dbschema.com/jdbc-drivers/InformixJdbcDriver.zip
            - unzip ./InformixJdbcDriver.zip

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_TEST=True
            - bash Deploy/Master/create-bundle OneView/ETL db2jcc4.jar DB2JccConfiguration.properties jdbc-4.50.4.1.jar bson-4.2.0.jar

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-tst-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/Master/upload-bundle OneView/ETL
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_TEST=True
            - export ENVIRONMENT=tst
            - bash --rcfile assume_role.rc -i Deploy/OneView/update-functions


        ##############################################################
        # stage branch targets
        ##############################################################

        CPT-stage:
        - step:
            name: Update Bundle
            caches:
                - cpt-stage-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_STAGE=True
            - bash Deploy/Master/create-bundle CPT

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-itg-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/CPT/upload-bundle
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_STAGE=True
            - export ENVIRONMENT=itg
            - bash --rcfile assume_role.rc -i Deploy/CPT/itg/update-functions

        Scheduler-stage:
        - step:
            name: Update Bundle
            caches:
                - scheduler-stage-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_STAGE=True
            - bash Deploy/Master/create-bundle Scheduler

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-itg-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/Master/upload-bundle Scheduler
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_STAGE=True
            - export ENVIRONMENT=itg
            - bash --rcfile assume_role.rc -i Deploy/Scheduler/update-functions


        ##############################################################
        # prod branch targets
        ##############################################################
        CPT-prod:
        - step:
            name: Send Deployment Request
            script:
            - echo "A CPT deployment has been triggered via BitBucket Pipelines. Please run the CPT-production target if you approve."

        CPT-production:
        - step:
            name: Update Bundle
            caches:
                - cpt-prod-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_PROD=True
            - bash Deploy/Master/create-bundle CPT

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-prd-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/CPT/upload-bundle
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_PROD=True
            - export ENVIRONMENT=prd
            - bash --rcfile assume_role.rc -i Deploy/CPT/prd/update-functions

        Scheduler-prod:
        - step:
            name: Send Deployment Request
            script:
            - echo "A Scheduler deployment has been triggered via BitBucket Pipelines. Please run the Scheduler-production target if you approve."

        Scheduler-production:
        - step:
            name: Update Bundle
            caches:
                - scheduler-prod-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_PROD=True
            - bash Deploy/Master/create-bundle Scheduler

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-prd-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/Master/upload-bundle Scheduler
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_PROD=True
            - export ENVIRONMENT=prd
            - bash --rcfile assume_role.rc -i Deploy/Scheduler/update-functions

        OneView-prod:
        - step:
            name: Send Deployment Request
            script:
            - echo "A OneView deployment has been triggered via BitBucket Pipelines. Please run the OneView-production target if you approve."

        OneView-production:
        - step:
            name: Update ETL Bundle
            caches:
                - oneview-prod-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines
            - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
            - export PATH="$VIRTUAL_ENV/bin:$PATH"

            # Install the DB2 JDBC driver
            - wget --no-check-certificate https://dbschema.com/jdbc-drivers/Db2JdbcDriver.zip
            - unzip ./Db2JdbcDriver.zip
            - echo "db2.jcc.charsetDecoderEncoder=3" >> ./DB2JccConfiguration.properties

            # Install the Informix JDBC driver
            - wget --no-check-certificate https://dbschema.com/jdbc-drivers/InformixJdbcDriver.zip
            - unzip ./InformixJdbcDriver.zip

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_PROD=True
            - bash Deploy/Master/create-bundle OneView/ETL db2jcc4.jar DB2JccConfiguration.properties jdbc-4.50.4.1.jar bson-4.2.0.jar

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-itg-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/Master/upload-bundle OneView/ETL
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_PROD=True
            - export ENVIRONMENT=prd
            - bash --rcfile assume_role.rc -i Deploy/OneView/update-functions

definitions:
  caches:
    build-dependencies: Environment
    test-dependencies: Environment

    cpt-master-create-database-dependencies: Environment
    cpt-master-update-bundle-dependencies: Environment
    scheduler-master-update-bundle-dependencies: Environment
    oneview-master-create-database-dependencies: Environment
    oneview-master-update-bundle-dependencies: Environment

    cpt-dev-update-bundle-dependencies: Environment
    scheduler-dev-update-bundle-dependencies: Environment
    oneview-dev-create-database-dependencies: Environment
    oneview-dev-update-bundle-dependencies: Environment

    cpt-test-update-bundle-dependencies: Environment
    scheduler-test-update-bundle-dependencies: Environment
    oneview-test-create-database-dependencies: Environment
    oneview-test-update-bundle-dependencies: Environment

    cpt-stage-update-bundle-dependencies: Environment
    scheduler-stage-update-bundle-dependencies: Environment

    cpt-prod-update-bundle-dependencies: Environment
    scheduler-prod-update-bundle-dependencies: Environment
    oneview-prod-update-bundle-dependencies: Environment
