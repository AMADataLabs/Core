image:
  name: 644454719059.dkr.ecr.us-east-1.amazonaws.com/datalabs-bitbucket-pipelines:1.3.0
  aws:
    access-key: $AWS_ACCESS_KEY_ID
    secret-key: $AWS_SECRET_ACCESS_KEY

options:
  docker: true

pipelines:
    branches:
        master:
        - step:
            name: Build and Auto-Merge to dev
            caches:
                - build-dependencies
            script:
            - bash Script/bitbucket-pipelines-build.sh
            - git config remote.origin.fetch "+refs/heads/*:refs/remotes/origin/*"
            - git fetch
            - git checkout dev
            - git merge -m 'Automatic merge from master to dev.' $BITBUCKET_COMMIT
            - git push origin dev
        dev:
        - step:
            name: Build dev
            caches:
                - build-dependencies
            script:
                - bash Script/bitbucket-pipelines-build.sh
        test:
        - step:
            name: Build test
            caches:
                - build-dependencies
            script:
                - bash Script/bitbucket-pipelines-build.sh
        stage:
        - step:
            name: Build stage
            caches:
                - build-dependencies
            script:
                - bash Script/bitbucket-pipelines-build.sh
        prod:
        - step:
            name: Build prod
            caches:
                - build-dependencies
            script:
                - bash Script/bitbucket-pipelines-build.sh
    custom:
        ##############################################################
        # global targets
        ##############################################################

        Test:
        - step:
            name: Unit and Lint Tests
            caches:
                - test-dependencies
            script:
                - bash Script/bitbucket-pipelines-test.sh


        ##############################################################
        # master branch targets
        ##############################################################

        AMC-master:
        - step:
            name: Update ETL Bundle
            caches:
                - amc-master-update-bundle-dependencies
            script:
            - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
            - >-
                bash Deploy/Master/deploy-lambda-function
                --environment sbx
                --project AMC
                --bundle Masterfile/AMC.zip
                --runtime Python
                --jdbc-driver Informix
        # - step:
        #     name: Update Lambda Functions
        #     script:
        #     - export ENABLE_FEATURE_DEV=True
        #     - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
        #     - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
        #     - export AWS_DEFAULT_REGION='us-east-1'
        #     - export ENVIRONMENT=sbx
        #     - bash Deploy/Master/update-functions -e sbx -s OneView -p Masterfile AMC=AMC.zip

        ContactID-master:
        - step:
            name: Update Bundle
            caches:
            - contactid-master-update-bundle-dependencies
            script:
            - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
            - >-
                bash Deploy/Master/deploy-lambda-function
                --environment sbx
                --project ContactID
                --bundle CustomerIntelligence/ContactID.zip
                --runtime Python
        - step:
            name: Update Lambda Functions
            script:
            - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
            - bash Deploy/Master/update-functions -e sbx -s DataLake -p CustomerIntelligence ContactID=ContactID.zip

        CPT-master:
        - parallel:
            - step:
                name: Update ETL Bundle
                caches:
                    - cpt-master-update-bundle-dependencies
                script:
                - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
                - >-
                    bash Deploy/Master/deploy-lambda-function
                    --environment sbx
                    --project CPT/API/ETL
                    --bundle CPT/API/ETL.zip
                    --runtime Python
            - step:
                name: Update Endpoint Bundle
                caches:
                    - cpt-master-update-bundle-dependencies
                script:
                - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
                - >-
                    bash Deploy/Master/deploy-lambda-function
                    --environment sbx
                    --project CPT/API/Endpoint
                    --bundle CPT/API/Endpoint.zip
                    --runtime Python
        - parallel:
            - step:
                name: Update Lambda Functions
                script:
                - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
                - >-
                    bash Deploy/Master/update-functions -e sbx -s CPT-API -p CPT/API
                    ETL=ETL.zip
                    Authorizer=Endpoint.zip
                    Endpoint=Endpoint.zip
                    BulkAuthorizer=Endpoint.zip
                    BulkEndpoint=Endpoint.zip

        HelloWorldJava-master:
        - parallel:
            - step:
                name: Update DAG Bundle
                caches:
                - scheduler-master-update-bundle-dependencies
                script:
                - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
                - >-
                    bash Deploy/Master/deploy-lambda-function
                    --environment sbx
                    --project HelloWorldJava/DAG
                    --bundle HelloWorldJava/DAG.zip
                    --runtime Python
            - step:
                name: Update Task Bundle
                caches:
                - scheduler-master-update-bundle-dependencies
                script:
                - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
                - >-
                    bash Deploy/Master/deploy-lambda-function
                    --environment sbx
                    --project HelloWorldJava/Task
                    --bundle HelloWorldJava/Task.zip
                    --runtime Java

                # Create the Docker image
                - export VERSION=`cat Build/HelloWorldJava/Task/VERSION`
                - cp Bundle/target/hello_world_java-${VERSION}.jar ./hello_world_java.jar
                - docker build -t hello_world_java-sbx -f Build/HelloWorldJava/Task/Dockerfile ./

                # Push the Docker image to ECR
                - export VERSION=`cat Build/HelloWorldJava/Task/VERSION`
                - pipe: atlassian/aws-ecr-push-image:1.5.0
                  variables:
                    AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID
                    AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY
                    AWS_DEFAULT_REGION: 'us-east-1'
                    IMAGE_NAME: 'hello_world_java-sbx'
                    TAGS: '$VERSION latest'
        - step:
            name: Update Lambda Functions
            script:
            - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
            - >-
                bash Deploy/Master/update-functions -e sbx -s DataLake -p HelloWorldJava
                HelloWorldJavaDAG=DAG.zip
                HelloWorldJavaTask=Task.jar

        OneView-master:
        - parallel:
            - step:
                name: Create Database
                caches:
                  - oneview-master-create-database-dependencies
                script:
                - bash Script/setup-virtual-environment OneView
                - export ENABLE_FEATURE_DEV=True
                - export DATABASE_NAME=oneview_content
                - export DATABASE_NAME_ADMIN=postgres
                - export DATABASE_BACKEND=postgresql+psycopg2
                - export DATABASE_HOST=$ONEVIEW_DATABASE_HOST_SANDBOX
                - export DATABASE_PORT=5432
                - export DATABASE_USERNAME=$ONEVIEW_DATABASE_USERNAME_SANDBOX
                - export DATABASE_PASSWORD=$ONEVIEW_DATABASE_PASSWORD_SANDBOX
                - bash Deploy/Master/create-database OneView
            - step:
                name: Update ETL Bundle
                caches:
                    - oneview-master-update-bundle-dependencies
                script:
                - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
                - >-
                    bash Deploy/Master/deploy-lambda-function
                    --environment sbx
                    --project OneView/ETL
                    --bundle OneView/ETL.zip
                    --runtime Python
                    --jdbc-driver DB2
                    --jdbc-driver Informix
            - step:
                name: Update API Bundle
                caches:
                    - oneview-master-update-bundle-dependencies
                script:
                - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
                - >-
                    bash Deploy/Master/deploy-lambda-function
                    --environment sbx
                    --project OneView/API
                    --bundle OneView/API.zip
                    --runtime Python
        - parallel:
            # - step:
            #     name: Migrate Database
            #     caches:
            #       - oneview-master-create-database-dependencies
            #     script:
            #     - bash Script/setup-virtual-environment Master/BitBucketPipelines
            #     - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
            #     - export PATH="$VIRTUAL_ENV/bin:$PATH"
            #     - Script/run.py Script/migrate-database --host $ONEVIEW_DATABASE_HOST_SANDBOX --name oneview --username $ONEVIEW_DATABASE_USERNAME_SANDBOX --password $ONEVIEW_DATABASE_PASSWORD_SANDBOX upgrade OneView
            - step:
                name: Update Lambda Functions
                script:
                - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
                - export ENVIRONMENT=sbx
                - bash Deploy/OneView/update-functions

        Organizations-master:
        - step:
            name: Update ETL Bundle
            caches:
                - organizations-master-update-bundle-dependencies
            script:
            - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
            - >-
                bash Deploy/Master/deploy-lambda-function
                --environment sbx
                --project CPT/Organizations
                --bundle IntelligentPlatform/OrganizationsETL.zip
                --runtime Python
                --jdbc-driver SQLServer

        Scheduler-master:
        - step:
            name: Update Component Package
            caches:
            - environment-master
            script:
            - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
            - bash Deploy/Master/deploy-python-package --project Scheduler --environment sbx
        - step:
            name: Update Bundle
            caches:
            - scheduler-master-update-bundle-dependencies
            script:
            - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
            - >-
                bash Deploy/Master/deploy-lambda-function
                --environment sbx
                --project Scheduler
                --bundle Scheduler.zip
                --runtime Python
        - step:
            name: Update Lambda Functions
            script:
            - bash Deploy/Master/setup-aws-cli -e sbx -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
            - >-
                bash Deploy/Master/update-functions -e sbx -s DataLake
                Scheduler=Scheduler.zip
                DAGProcessor=Scheduler.zip
                TaskProcessor=Scheduler.zip


        ##############################################################
        # dev branch targets
        ##############################################################

        CPT-dev:
        - parallel:
            - step:
                name: Update ETL Bundle
                caches:
                - cpt-dev-update-bundle-dependencies
                script:
                - >-
                    bash Deploy/Master/setup-aws-cli -e dev -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
                    -I $AWS_ACCESS_KEY_ID_ROLE -S $AWS_SECRET_ACCESS_KEY_ROLE
                - >-
                    bash --rcfile assume_role.rc -i Deploy/Master/deploy-lambda-function
                    --environment dev
                    --project CPT/API/ETL
                    --bundle CPT/API/ETL.zip
                    --runtime Python
            - step:
                name: Update Endpoint Bundle
                caches:
                - cpt-dev-update-bundle-dependencies
                script:
                - >-
                    bash Deploy/Master/setup-aws-cli -e dev -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
                    -I $AWS_ACCESS_KEY_ID_ROLE -S $AWS_SECRET_ACCESS_KEY_ROLE
                - >-
                    bash --rcfile assume_role.rc -i Deploy/Master/deploy-lambda-function
                    --environment dev
                    --project CPT/API/Endpoint
                    --bundle CPT/API/Endpoint.zip
                    --runtime Python
        - step:
            name: Update Lambda Functions
            script:
            # Update Lambda functions
            - export ENABLE_FEATURE_DEV=True
            - export ENVIRONMENT=dev
            - >-
                bash Deploy/Master/setup-aws-cli -e dev -i $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY
                -I $AWS_ACCESS_KEY_ID_ROLE -S $AWS_SECRET_ACCESS_KEY_ROLE
            - >-
                bash --rcfile assume_role.rc -i Deploy/Master/update-functions -e dev -s CPT-API -p CPT/API
                Endpoint=ETL.zip
                Authorizer=Endpoint.zip
                ETL=Endpoint.zip
                BulkAuthorizer=Endpoint.zip
                BulkEndpoint=Endpoint.zip

        OneView-dev:
        - parallel:
            # - step:
            #     name: Create Database
            #     caches:
            #       - oneview-dev-create-database-dependencies
            #     script:
            #     - bash Script/setup-virtual-environment OneView
            #     - export ENABLE_FEATURE_DEV=True
            #     - export DATABASE_NAME=oneview_content
            #     - export DATABASE_NAME_ADMIN=postgres
            #     - export DATABASE_BACKEND=postgresql+psycopg2
            #     - export DATABASE_HOST=$ONEVIEW_DATABASE_HOST_DEV
            #     - export DATABASE_PORT=5432
            #     - export DATABASE_USERNAME=$ONEVIEW_DATABASE_USERNAME_DEV
            #     - export DATABASE_PASSWORD=$ONEVIEW_DATABASE_PASSWORD_DEV
            #     - bash Deploy/Master/create-database OneView
            - step:
                name: Update ETL Bundle
                caches:
                    - oneview-dev-update-bundle-dependencies
                script:
                # Setup virtual environment
                - bash Script/setup-virtual-environment Master/BitBucketPipelines
                - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
                - export PATH="$VIRTUAL_ENV/bin:$PATH"

                # Setup AWS CLI
                - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
                - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
                - export AWS_DEFAULT_REGION='us-east-1'

                # Install the DB2 JDBC driver
                - aws s3 cp s3://ama-sbx-datalake-lambda-us-east-1/JDBC/Db2JdbcDriver.zip Db2JdbcDriver.zip
                - unzip ./Db2JdbcDriver.zip

                # Install the Informix JDBC driver
                - aws s3 cp s3://ama-sbx-datalake-lambda-us-east-1/JDBC/InformixJdbcDriver.zip InformixJdbcDriver.zip
                - unzip ./InformixJdbcDriver.zip
                - echo "db2.jcc.charsetDecoderEncoder=3" >> ./DB2JccConfiguration.properties

                # Assume maintenance role
                - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
                - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
                - aws configure set profile.apigw.region us-east-1
                - aws configure set profile.apigw.output json
                - bash Script/apigw_assume_role.sh dev | grep source > assume_role.rc

                # Create the code bundle
                - export ENABLE_FEATURE_DEV=True
                - >-
                    bash Deploy/Master/create-python-bundle --project --project OneView/ETL
                    -f db2jcc4.jar -f DB2JccConfiguration.properties -f jdbc-4.50.4.1.jar -f bson-4.2.0.jar

                # Upload the code bundle
                - export AWS_S3_BUCKET='ama-dev-datalake-lambda-us-east-1'
                - bash --rcfile assume_role.rc -i Deploy/Master/upload-python-bundle OneView/ETL
            - step:
                name: Update API Bundle
                caches:
                    - oneview-dev-update-bundle-dependencies
                script:
                # Setup virtual environment
                - bash Script/setup-virtual-environment Master/BitBucketPipelines
                - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
                - export PATH="$VIRTUAL_ENV/bin:$PATH"

                # Assume maintenance role
                - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
                - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
                - aws configure set profile.apigw.region us-east-1
                - aws configure set profile.apigw.output json
                - bash Script/apigw_assume_role.sh dev | grep source > assume_role.rc

                # Create the code bundle
                - export ENABLE_FEATURE_DEV=True
                - bash Deploy/Master/create-python-bundle --project OneView/API

                # Upload the code bundle
                - export AWS_S3_BUCKET='ama-dev-datalake-lambda-us-east-1'
                - bash --rcfile assume_role.rc -i Deploy/Master/upload-python-bundle OneView/API
        - parallel:
            # - step:
            #     name: Migrate Database
            #     caches:
            #       - oneview-dev-create-database-dependencies
            #     script:
            #     - bash Script/setup-virtual-environment Master/BitBucketPipelines
            #     - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
            #     - export PATH="$VIRTUAL_ENV/bin:$PATH"
            #     - bash Script/migrate-database --host $ONEVIEW_DATABASE_HOST_DEV --name oneview --username $ONEVIEW_DATABASE_USERNAME_DEV --password $ONEVIEW_DATABASE_PASSWORD_DEV upgrade OneView
            - step:
                name: Update Lambda Functions
                script:
                # Assume maintenance role
                - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
                - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
                - aws configure set profile.apigw.region us-east-1
                - aws configure set profile.apigw.output json
                - bash Script/apigw_assume_role.sh dev | grep source > assume_role.rc

                # Update Lambda functions
                - export ENABLE_FEATURE_DEV=True
                - export ENVIRONMENT=dev
                - bash --rcfile assume_role.rc -i Deploy/OneView/update-functions

        Organizations-dev:
        - step:
            name: Update ETL Bundle
            caches:
                - organizations-dev-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines
            - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
            - export PATH="$VIRTUAL_ENV/bin:$PATH"

            # Setup AWS CLI
            - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
            - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
            - export AWS_DEFAULT_REGION='us-east-1'

            # Install the SQL Server JDBC driver
            - aws s3 cp s3://ama-sbx-datalake-lambda-us-east-1/JDBC/SqlServerJdbcDriver.zip SqlServerJdbcDriver.zip
            - unzip ./SqlServerJdbcDriver.zip
            - mv sqljdbc_10.2\\enu/mssql-jdbc-10.2.0.jre8.jar mssql-jdbc-10.2.0.jre8.jar

            # Create the code bundle
            - export ENABLE_FEATURE_DEV=True
            - bash Deploy/Master/create-python-bundle --project CPT/Organizations -f mssql-jdbc-10.2.0.jre8.jar

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh dev | grep source > assume_role.rc

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-dev-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/Master/upload-python-bundle IntelligentPlatform/OrganizationsETL
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh dev | grep source > assume_role.rc

            # Update Lambda functions
            - >-
                bash --rcfile assume_role.rc -i Deploy/Master/update-functions -e dev -s AIP -p IntelligentPlatform
                OrganizationsETL=OrganizationsETL.zip

        Scheduler-dev:
        - step:
            name: Update Bundle
            caches:
                - scheduler-dev-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh dev | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_DEV=True
            - bash Deploy/Master/create-python-bundle --project Scheduler

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-dev-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/Master/upload-python-bundle Scheduler
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh dev | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_DEV=True
            - export ENVIRONMENT=dev
            - >-
                bash --rcfile assume_role.rc -i Deploy/Master/update-functions -e dev -s DataLake
                Scheduler=Scheduler.zip
                DAGProcessor=Scheduler.zip
                TaskProcessor=Scheduler.zip


        ##############################################################
        # test branch targets
        ##############################################################

        CPT-test:
        - step:
            name: Update Bundle
            caches:
                - cpt-test-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_TEST=True
            - bash Deploy/Master/create-python-bundle --project CPT

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-tst-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/CPT/upload-bundle
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_TEST=True
            - export ENVIRONMENT=tst
            - >-
                bash --rcfile assume_role.rc -i Deploy/Master/update-functions -e tst -s CPT-API -p CPT
                Authorizer=ETL.zip
                Endpoint=ETL.zip
                ETL=ETL.zip
                BulkAuthorizer=ETL.zip
                BulkEndpoint=ETL.zip

        OneView-test:
        - parallel:
            - step:
                name: Update ETL Bundle
                caches:
                    - oneview-test-update-bundle-dependencies
                script:
                # Setup virtual environment
                - bash Script/setup-virtual-environment Master/BitBucketPipelines
                - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
                - export PATH="$VIRTUAL_ENV/bin:$PATH"

                # Setup AWS CLI
                - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
                - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
                - export AWS_DEFAULT_REGION='us-east-1'

                # Install the DB2 JDBC driver
                - aws s3 cp s3://ama-sbx-datalake-lambda-us-east-1/JDBC/Db2JdbcDriver.zip Db2JdbcDriver.zip
                - unzip ./Db2JdbcDriver.zip

                # Install the Informix JDBC driver
                - aws s3 cp s3://ama-sbx-datalake-lambda-us-east-1/JDBC/InformixJdbcDriver.zip InformixJdbcDriver.zip
                - unzip ./InformixJdbcDriver.zip
                - echo "db2.jcc.charsetDecoderEncoder=3" >> ./DB2JccConfiguration.properties

                # Assume maintenance role
                - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
                - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
                - aws configure set profile.apigw.region us-east-1
                - aws configure set profile.apigw.output json
                - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

                # Create the code bundle
                - export ENABLE_FEATURE_TEST=True
                - >-
                    bash Deploy/Master/create-python-bundle --project OneView/ETL
                    -f db2jcc4.jar -f DB2JccConfiguration.properties -f jdbc-4.50.4.1.jar -f bson-4.2.0.jar

                # Upload the code bundle
                - export AWS_S3_BUCKET='ama-tst-datalake-lambda-us-east-1'
                - bash --rcfile assume_role.rc -i Deploy/Master/upload-python-bundle OneView/ETL
            - step:
                name: Update API Bundle
                caches:
                    - oneview-test-update-bundle-dependencies
                script:
                # Setup virtual environment
                - bash Script/setup-virtual-environment Master/BitBucketPipelines
                - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
                - export PATH="$VIRTUAL_ENV/bin:$PATH"

                # Assume maintenance role
                - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
                - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
                - aws configure set profile.apigw.region us-east-1
                - aws configure set profile.apigw.output json
                - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

                # Create the code bundle
                - export ENABLE_FEATURE_TEST=True
                - bash Deploy/Master/create-python-bundle --project OneView/API

                # Upload the code bundle
                - export AWS_S3_BUCKET='ama-tst-datalake-lambda-us-east-1'
                - bash --rcfile assume_role.rc -i Deploy/Master/upload-python-bundle OneView/API
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_TEST=True
            - export ENVIRONMENT=tst
            - bash --rcfile assume_role.rc -i Deploy/OneView/update-functions

        Organizations-test:
        - step:
            name: Update ETL Bundle
            caches:
                - organizations-test-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines
            - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
            - export PATH="$VIRTUAL_ENV/bin:$PATH"

            # Setup AWS CLI
            - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
            - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
            - export AWS_DEFAULT_REGION='us-east-1'

            # Install the SQL Server JDBC driver
            - aws s3 cp s3://ama-sbx-datalake-lambda-us-east-1/JDBC/SqlServerJdbcDriver.zip SqlServerJdbcDriver.zip
            - unzip ./SqlServerJdbcDriver.zip
            - mv sqljdbc_10.2\\enu/mssql-jdbc-10.2.0.jre8.jar mssql-jdbc-10.2.0.jre8.jar

            # Create the code bundle
            - export ENABLE_FEATURE_TEST=True
            - bash Deploy/Master/create-python-bundle --project CPT/Organizations -f mssql-jdbc-10.2.0.jre8.jar

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-tst-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/Master/upload-python-bundle IntelligentPlatform/OrganizationsETL
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

            # Update Lambda functions
            - >-
                bash --rcfile assume_role.rc -i Deploy/Master/update-functions -e tst -s AIP -p IntelligentPlatform
                OrganizationsETL=OrganizationsETL.zip

        Scheduler-test:
        - step:
            name: Update Bundle
            caches:
                - scheduler-test-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_TEST=True
            - bash Deploy/Master/create-python-bundle --project Scheduler

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-tst-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/Master/upload-python-bundle Scheduler
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh tst | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_TEST=True
            - export ENVIRONMENT=tst
            - >-
                bash --rcfile assume_role.rc -i Deploy/Master/update-functions -e tst -s DataLake
                Scheduler=Scheduler.zip
                DAGProcessor=Scheduler.zip
                TaskProcessor=Scheduler.zip


        ##############################################################
        # stage branch targets
        ##############################################################

        CPT-stage:
        - step:
            name: Update Bundle
            caches:
                - cpt-stage-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_STAGE=True
            - bash Deploy/Master/create-python-bundle --project CPT

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-itg-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/CPT/upload-bundle
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_STAGE=True
            - export ENVIRONMENT=itg
            - >-
                bash --rcfile assume_role.rc -i Deploy/Master/update-functions -e itg -s CPT-API -p CPT
                Authorizer=ETL.zip
                Endpoint=ETL.zip
                ETL=ETL.zip
                BulkAuthorizer=ETL.zip
                BulkEndpoint=ETL.zip

        Scheduler-stage:
        - step:
            name: Update Bundle
            caches:
                - scheduler-stage-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_STAGE=True
            - bash Deploy/Master/create-python-bundle --project Scheduler

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-itg-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/Master/upload-python-bundle Scheduler
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_STAGE=True
            - export ENVIRONMENT=itg
            - >-
                bash --rcfile assume_role.rc -i Deploy/Master/update-functions -e itg -s DataLake
                Scheduler=Scheduler.zip
                DAGProcessor=Scheduler.zip
                TaskProcessor=Scheduler.zip


        ##############################################################
        # prod branch targets
        ##############################################################
        CPT-prod:
        - step:
            name: Send Deployment Request
            script:
            - echo "A CPT deployment has been triggered via BitBucket Pipelines. Please run the CPT-production target if you approve."

        CPT-production:
        - step:
            name: Update Bundle
            caches:
                - cpt-prod-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_PROD=True
            - bash Deploy/Master/create-python-bundle --project CPT

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-prd-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/CPT/upload-bundle
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_PROD=True
            - export ENVIRONMENT=prd
            - >-
                bash --rcfile assume_role.rc -i Deploy/Master/update-functions -e prd -s CPT-API -p CPT
                Authorizer=ETL.zip
                Endpoint=ETL.zip
                ETL=ETL.zip
                BulkAuthorizer=ETL.zip
                BulkEndpoint=ETL.zip

        OneView-prod:
        - step:
            name: Send Deployment Request
            script:
            - echo "A OneView deployment has been triggered via BitBucket Pipelines. Please run the OneView-production target if you approve."

        OneView-production:
        - parallel:
            - step:
                name: Update ETL Bundle
                caches:
                    - oneview-prod-update-bundle-dependencies
                script:
                # Setup virtual environment
                - bash Script/setup-virtual-environment Master/BitBucketPipelines
                - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
                - export PATH="$VIRTUAL_ENV/bin:$PATH"

                # Setup AWS CLI
                - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
                - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
                - export AWS_DEFAULT_REGION='us-east-1'

                # Install the DB2 JDBC driver
                - aws s3 cp s3://ama-sbx-datalake-lambda-us-east-1/JDBC/Db2JdbcDriver.zip Db2JdbcDriver.zip
                - unzip ./Db2JdbcDriver.zip

                # Install the Informix JDBC driver
                - aws s3 cp s3://ama-sbx-datalake-lambda-us-east-1/JDBC/InformixJdbcDriver.zip InformixJdbcDriver.zip
                - unzip ./InformixJdbcDriver.zip
                - echo "db2.jcc.charsetDecoderEncoder=3" >> ./DB2JccConfiguration.properties

                # Assume maintenance role
                - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
                - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
                - aws configure set profile.apigw.region us-east-1
                - aws configure set profile.apigw.output json
                - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

                # Create the code bundle
                - export ENABLE_FEATURE_PROD=True
                - >-
                    bash Deploy/Master/create-python-bundle --project OneView/ETL
                    -f db2jcc4.jar -f DB2JccConfiguration.properties -f jdbc-4.50.4.1.jar -f bson-4.2.0.jar

                # Upload the code bundle
                - export AWS_S3_BUCKET='ama-prd-datalake-lambda-us-east-1'
                - bash --rcfile assume_role.rc -i Deploy/Master/upload-python-bundle OneView/ETL
            - step:
                name: Update API Bundle
                caches:
                    - oneview-prod-update-bundle-dependencies
                script:
                # Setup virtual environment
                - bash Script/setup-virtual-environment Master/BitBucketPipelines
                - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
                - export PATH="$VIRTUAL_ENV/bin:$PATH"

                # Assume maintenance role
                - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
                - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
                - aws configure set profile.apigw.region us-east-1
                - aws configure set profile.apigw.output json
                - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

                # Create the code bundle
                - export ENABLE_FEATURE_PROD=True
                - bash Deploy/Master/create-python-bundle --project OneView/API

                # Upload the code bundle
                - export AWS_S3_BUCKET='ama-prd-datalake-lambda-us-east-1'
                - bash --rcfile assume_role.rc -i Deploy/Master/upload-python-bundle OneView/API
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_PROD=True
            - export ENVIRONMENT=prd
            - bash --rcfile assume_role.rc -i Deploy/OneView/update-functions

        Organizations-prod:
        - step:
            name: Send Deployment Request
            script:
            - echo "A Organizations deployment has been triggered via BitBucket Pipelines. Please run the Organizations-production target if you approve."

        Organizations-production:
        - step:
            name: Update ETL Bundle
            caches:
                - organizations-prod-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines
            - export VIRTUAL_ENV=${PWD}/Environment/Master/BitBucketPipelines
            - export PATH="$VIRTUAL_ENV/bin:$PATH"

            # Setup AWS CLI
            - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
            - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
            - export AWS_DEFAULT_REGION='us-east-1'

            # Install the SQL Server JDBC driver
            - aws s3 cp s3://ama-sbx-datalake-lambda-us-east-1/JDBC/SqlServerJdbcDriver.zip SqlServerJdbcDriver.zip
            - unzip ./SqlServerJdbcDriver.zip
            - mv sqljdbc_10.2\\enu/mssql-jdbc-10.2.0.jre8.jar mssql-jdbc-10.2.0.jre8.jar

            # Create the code bundle
            - export ENABLE_FEATURE_PROD=True
            - bash Deploy/Master/create-python-bundle --project CPT/Organizations -f mssql-jdbc-10.2.0.jre.jar

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-prd-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/Master/upload-python-bundle IntelligentPlatform/OrganizationsETL
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Update Lambda functions
            - >-
                bash --rcfile assume_role.rc -i Deploy/Master/update-functions -e prd -s AIP -p IntelligentPlatform
                OrganizationsETL=OrganizationsETL.zip

        Scheduler-prod:
        - step:
            name: Send Deployment Request
            script:
            - echo "A Scheduler deployment has been triggered via BitBucket Pipelines. Please run the Scheduler-production target if you approve."

        Scheduler-production:
        - step:
            name: Update Bundle
            caches:
                - scheduler-prod-update-bundle-dependencies
            script:
            # Setup virtual environment
            - bash Script/setup-virtual-environment Master/BitBucketPipelines

            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Create the code bundle
            - export ENABLE_FEATURE_PROD=True
            - bash Deploy/Master/create-python-bundle --project Scheduler

            # Upload the code bundle
            - export AWS_S3_BUCKET='ama-prd-datalake-lambda-us-east-1'
            - bash --rcfile assume_role.rc -i Deploy/Master/upload-python-bundle Scheduler
        - step:
            name: Update Lambda Functions
            script:
            # Assume maintenance role
            - aws configure set profile.apigw.aws_access_key_id $AWS_ACCESS_KEY_ID_ROLE
            - aws configure set profile.apigw.aws_secret_access_key $AWS_SECRET_ACCESS_KEY_ROLE
            - aws configure set profile.apigw.region us-east-1
            - aws configure set profile.apigw.output json
            - bash Script/apigw_assume_role.sh prd | grep source > assume_role.rc

            # Update Lambda functions
            - export ENABLE_FEATURE_PROD=True
            - export ENVIRONMENT=prd
            - >-
                bash --rcfile assume_role.rc -i Deploy/Master/update-functions -e prd -s DataLake
                Scheduler=Scheduler.zip
                DAGProcessor=Scheduler.zip
                TaskProcessor=Scheduler.zip

definitions:
  caches:
    build-dependencies: Environment
    test-dependencies: Environment

    environment-master: Environment
    amc-master-update-bundle-dependencies: Environment
    contactid-master-update-bundle-dependencies: Environment
    cpt-master-create-database-dependencies: Environment
    cpt-master-update-bundle-dependencies: Environment
    oneview-master-create-database-dependencies: Environment
    oneview-master-update-bundle-dependencies: Environment
    organizations-master-update-bundle-dependencies: Environment
    scheduler-master-update-bundle-dependencies: Environment

    cpt-dev-update-bundle-dependencies: Environment
    oneview-dev-create-database-dependencies: Environment
    oneview-dev-update-bundle-dependencies: Environment
    organizations-dev-update-bundle-dependencies: Environment
    scheduler-dev-update-bundle-dependencies: Environment

    cpt-test-update-bundle-dependencies: Environment
    oneview-test-create-database-dependencies: Environment
    oneview-test-update-bundle-dependencies: Environment
    organizations-test-update-bundle-dependencies: Environment
    scheduler-test-update-bundle-dependencies: Environment

    cpt-stage-update-bundle-dependencies: Environment
    scheduler-stage-update-bundle-dependencies: Environment

    cpt-prod-update-bundle-dependencies: Environment
    oneview-prod-update-bundle-dependencies: Environment
    organizations-prod-update-bundle-dependencies: Environment
    scheduler-prod-update-bundle-dependencies: Environment
