---
VERICRE_DATA_STEWARD:
    GLOBAL:
        ACCOUNT: '191296302136'
        ENVIRONMENT: dev

        PLATFORM_DB_BACKEND: postgresql+psycopg2
        PLATFORM_DB_HOST: vericre-dev-au-aurora-cluster.cluster-cwp4vd8mllvz.us-east-1.rds.amazonaws.com
        PLATFORM_DB_NAME: vericre
        PLATFORM_DB_PASSWORD: '{{platform_db_password}}'
        PLATFORM_DB_PORT: '5432'
        PLATFORM_DB_USERNAME: vericreadmin

        S3_BASE_PATH: AMA/VeriCre/DataSteward

        S3_CACHE_CLASS: datalabs.etl.dag.cache.s3.S3TaskDataCache

        S3_INGESTED_DATA_BUCKET: ama-${ENVIRONMENT}-datalake-ingest-us-east-1
        S3_PROCESSED_DATA_BUCKET: ama-${ENVIRONMENT}-datalake-process-us-east-1
        S3_VERICRE_DATA_BUCKET: ama-${ENVIRONMENT}-vericre-us-east-1
    DAG:
        DAG_CLASS: datalabs.etl.dag.vericre.data_steward.DAG
        DAG_EXECUTOR_CLASS: datalabs.etl.dag.execute.awslambda.LambdaDAGExecutorTask
        DAG_STATE:
            CLASS: datalabs.etl.dag.state.dynamodb.DAGState
            LOCK_TABLE: DataLake-scheduler-locks-${ENVIRONMENT}
            STATE_TABLE: DataLake-dag-state-${ENVIRONMENT}
        DAG_TOPIC_ARN: arn:aws:sns:us-east-1:${ACCOUNT}:DataLake-${ENVIRONMENT}-DAGProcessor
        ENVIRONMENT: ${ENVIRONMENT}
        LAMBDA_FUNCTION: VeriCre-${ENVIRONMENT}-DataStewardDAG
        STATUS_NOTIFICATION_WEB_HOOK: https://amatoday.webhook.office.com/webhookb2/5cf63a5c-89ba-4724-b44f-8643269bae92@11fe67f6-fd54-4981-925f-94c36ed7b086/IncomingWebhook/1d0f20d047d042d68d01a8d855787009/b9b792dd-3637-4256-8a95-72d5d956f908
        TASK_EXECUTOR_CLASS: datalabs.etl.dag.execute.awslambda.LambdaTaskExecutorTask
        TASK_TOPIC_ARN: arn:aws:sns:us-east-1:${ACCOUNT}:DataLake-${ENVIRONMENT}-TaskProcessor
    EXTRACT_NEW_APPLICATION_PATHS_TODAY:
        DATABASE_BACKEND: ${PLATFORM_DB_BACKEND}
        DATABASE_HOST: ${PLATFORM_DB_HOST}
        DATABASE_NAME: ${PLATFORM_DB_NAME}
        DATABASE_PASSWORD: ${PLATFORM_DB_PASSWORD}
        DATABASE_PORT: ${PLATFORM_DB_PORT}
        DATABASE_USERNAME: ${PLATFORM_DB_USERNAME}
        SQL: "
            SELECT d.user || '/' || document_path || '/' || document_name AS path FROM (
                SELECT document_path,document_name,a.user,updated_datetime FROM (
                    SELECT document_id,application.user,to_timestamp(updated_at / 1000) AS updated_datetime
                    FROM application
                    JOIN application_document_mapping ON application.id = application_id
                    WHERE status IN ('CREATED_N_SUBMITTED', 'RESUBMITTED_TO_DS')
                ORDER BY updated_datetime DESC
                ) AS a
                JOIN document ON a.document_id = document.id
            ) AS d
            WHERE updated_datetime
            BETWEEN timestamp '%Y-%m-%d %H:%M:%S' - interval '1 day'
            AND timestamp '%Y-%m-%d %H:%M:%S'
            ORDER BY path;
            SELECT d.user,me_number,name FROM (
                SELECT a.user,ama_me_number AS me_number,ama_name AS name,updated_datetime FROM (
                SELECT application.user,to_timestamp(updated_at / 1000) AS updated_datetime
                FROM application
                JOIN application_document_mapping ON application.id = application_id
                WHERE status IN ('CREATED_N_SUBMITTED', 'RESUBMITTED_TO_DS')
                ORDER BY updated_datetime DESC
            ) AS a
            JOIN \"user\" as u ON a.user = u.id
            ) AS d
            WHERE updated_datetime
            BETWEEN timestamp '%Y-%m-%d %H:%M:%S' - interval '1 day'
            AND timestamp '%Y-%m-%d %H:%M:%S'
            ORDER BY d.user;
        "

        CACHE_OUTPUT_CLASS: ${S3_CACHE_CLASS}
        CACHE_OUTPUT_BASE_PATH: ${S3_BASE_PATH}
        CACHE_OUTPUT_BUCKET: ${S3_VERICRE_DATA_BUCKET}
        CACHE_OUTPUT_FILES: new_application_paths.csv,user_id_me_number_name_map.csv
    EXTRACT_USER_UPLOADS:
        CACHE_CLASS: ${S3_CACHE_CLASS}

        CACHE_INPUT_BASE_PATH: ${S3_BASE_PATH}
        CACHE_INPUT_BUCKET: ${S3_VERICRE_DATA_BUCKET}
        CACHE_INPUT_FILES: new_application_paths.csv

        BASE_PATH: ''
        BUCKET: ${S3_VERICRE_DATA_BUCKET}
        IGNORE_FILES_HEADER: 'True'
        INCLUDE_DATESTAMP: 'False'
        INCLUDE_NAMES: 'True'

        CACHE_OUTPUT_BASE_PATH: ${S3_BASE_PATH}
        CACHE_OUTPUT_BUCKET: ${S3_VERICRE_DATA_BUCKET}
        CACHE_OUTPUT_FILES: new_application_data.pkl
    SEND_USER_UPLOADS:
        CACHE_INPUT_CLASS: ${S3_CACHE_CLASS}
        CACHE_INPUT_BASE_PATH: ${S3_BASE_PATH}
        CACHE_INPUT_BUCKET: ${S3_VERICRE_DATA_BUCKET}
        CACHE_INPUT_FILES: new_application_data.zip,user_id_me_number_name_map.csv

        FILES: new_application_data.zip,user_id_me_number_name_map.csv
        MESSAGE: "
            This is an automated email from DataLabs.

            Attached are the user uploads for new or resubmitted Serf-Service Licensing
            applications for the past 24 hours.
        "
        SUBJECT: Self-Service Licensing New User Uploads - %Y-%m-%d
        TO_ADDRESSES: datalabs@ama-assn.org
    ZIP_USER_UPLOADS:
        CACHE_CLASS: ${S3_CACHE_CLASS}

        CACHE_INPUT_BASE_PATH: ${S3_BASE_PATH}
        CACHE_INPUT_BUCKET: ${S3_VERICRE_DATA_BUCKET}
        CACHE_INPUT_FILES: new_application_data.pkl

        CACHE_OUTPUT_BASE_PATH: ${S3_BASE_PATH}
        CACHE_OUTPUT_BUCKET: ${S3_VERICRE_DATA_BUCKET}
        CACHE_OUTPUT_FILES: new_application_data.zip
